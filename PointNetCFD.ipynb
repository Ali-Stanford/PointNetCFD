{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PointNetCFD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Point-cloud deep learning for prediction of fluid flow fields on irregular geometries (supervised learning)\n",
        "\n",
        "**Authors:** Ali Kashefi (kashefi@stanford.edu) and Davis Rempe (drempe@stanford.edu)<br>\n",
        "**Description:** Implementation of PointNet for *supervised learning* of computational mechanics on domains with irregular geometries <br>\n",
        "**Version:** 1.0 <br>\n",
        "**Guidance:** We recommend opening and running the code on **[Google Colab](https://research.google.com/colaboratory)** as a first try. \n"
      ],
      "metadata": {
        "id": "BS22gRSyC-_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Citation\n",
        "If you use the code, plesae cite the following journal paper: <br>\n",
        "**[A point-cloud deep learning framework for prediction of fluid flow fields on irregular geometries](https://aip.scitation.org/doi/full/10.1063/5.0033376)**\n",
        "\n",
        "@article{kashefi2021PointNetCFD, <br>\n",
        "author = {Kashefi, Ali  and Rempe, Davis  and Guibas, Leonidas J. }, <br>\n",
        "title = {A point-cloud deep learning framework for prediction of fluid flow fields on irregular geometries}, <br>\n",
        "journal = {Physics of Fluids}, <br>\n",
        "volume = {33}, <br>\n",
        "number = {2}, <br>\n",
        "pages = {027104}, <br>\n",
        "year = {2021}, <br>\n",
        "doi = {10.1063/5.0033376},}"
      ],
      "metadata": {
        "id": "blfvDXEpAhGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#1. Introduction\n",
        "We provide the implementation of [PointNet](https://arxiv.org/abs/1612.00593) for prediction of quantities of interest in the area of computational mechanics on domains with irregular geometries. Specifically, we present the example of flow past a cylinder with various shapes for its cross sections. We hope that this simple example motivates other researchers to use [PointNet](https://arxiv.org/abs/1612.00593) for other areas of computational mechanics and physics such as compressible flows, solid mechanics, etc. <br>\n",
        "To make the code usable for everyone (even with a moderate knowledge of deep learning), we implement the code using [Keras](https://keras.io/). We explain the procedure step by step.<br>\n",
        "We strongly recommend users to read the journal paber of **\"A point-cloud deep learning framework for prediction of fluid flow fields on irregular geometries\"** (https://aip.scitation.org/doi/full/10.1063/5.0033376). <br>\n",
        "You might also find free versions of this article on [arXiv](http://arxiv-export-lb.library.cornell.edu/abs/2010.09469) or [ResearchGates](https://www.researchgate.net/publication/349544773_A_point-cloud_deep_learning_framework_for_prediction_of_fluid_flow_fields_on_irregular_geometries/stats).<br>\n"
      ],
      "metadata": {
        "id": "iaJKq0B9EHyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Google Colab \n",
        "We strongly recommend to run the code on Google Colab as a first try. Here is a link to Colab https://research.google.com/colaboratory. In this way, you will not need to install different libraries. Moreover you do not need to be worried about matching the required libraries."
      ],
      "metadata": {
        "id": "CcNyCoFYGy_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Flow past a cylinder (steady-state)\n",
        "We consider laminar steady-state flow past a cylinder with different shapes for its cross sections. In fact, we consider the same example discussed in the journal paper. Please see *Figure 2* and *Figure 3* of the [journal paper](https://aip.scitation.org/doi/full/10.1063/5.0033376) for the generated meshes and flow fields. For geometries of the cross section of the cylinder, we use those geometries described in *Table 1* of the [journal paper](https://aip.scitation.org/doi/full/10.1063/5.0033376). However, due to reducing the data size and making it possible to run the code in a reasonable amount of time on Google Colab, we only consider \"circle,\" \"equilateral hexagon,\" \"equilateral pentagon,\" \"square,\" and \"equilateral triangle.\" For the fluid and flow properties such as density, viscosity, and the magnitude of free stream velocity, please see the section of *Governing equations of fluid dynamics* of the [journal paper](https://aip.scitation.org/doi/full/10.1063/5.0033376) for further details. "
      ],
      "metadata": {
        "id": "9-nHjRv_JhkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. TensorFlow Setup\n",
        "We use TensorFlow 1.x. To set this, please run the following code in the Google Colab environment. <br>\n",
        "If you would like to run the code on your personal laptop or on computing clusters, we recommend to use [Miniconda](https://docs.conda.io/en/latest/miniconda.html) to set an appropriate environment. "
      ],
      "metadata": {
        "id": "SQOikECbKSPR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccb-_8sZ6f2Z"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Importing libraries\n",
        "As a first step, we import the necessary libraries. We use [Matplotlib](https://matplotlib.org/) for visualization purposes and [NumPy](https://numpy.org/) for computing on arrays."
      ],
      "metadata": {
        "id": "HGyw0EmlIMcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import linecache\n",
        "import math\n",
        "from operator import itemgetter\n",
        "import numpy as np\n",
        "from numpy import zeros\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
        "plt.rcParams['font.size'] = '12'\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.layers import Input\n",
        "from tensorflow.python.keras import optimizers\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Reshape\n",
        "from tensorflow.python.keras.layers import Convolution1D, MaxPooling1D, BatchNormalization\n",
        "from tensorflow.python.keras.layers import Lambda, concatenate"
      ],
      "metadata": {
        "id": "F0GgDaACIeuC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Importing data\n",
        "For your convinient, we have already prepared data as a numpy array. The data for this specific test case is the spatial coordinates of the finite volume (or finite element) grids and the values of velocity and pressure fields on those grid points. The spatial coordinates are the input of PointNet and the velocity (in the *x* and *y* directions) and pressure fields are the output of PointNet. Here, our focus is on 2D cases."
      ],
      "metadata": {
        "id": "YjMn5rBkLEK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
   
        "!git clone https://github.com/Ali-Stanford/PointNetCFD.git\n",
        "\n",
        "Data = np.load('PointNetCFD/Data.npy')\n",
        "data_number = Data.shape[0]\n",
        "\n",
        "print('Number of data is:')\n",
        "print(data_number)\n",
        "\n",
        "point_numbers = 1024\n",
        "space_variable = 2 # 2 in 2D (x,y) and 3 in 3D (x,y,z)\n",
        "cfd_variable = 3 # (u, v, p); which are the x-component of velocity, y-component of velocity, and pressure fields\n",
        "\n",
        "input_data = zeros([data_number,point_numbers,space_variable],dtype='f')\n",
        "output_data = zeros([data_number,point_numbers,cfd_variable],dtype='f')\n",
        "\n",
        "for i in range(data_number):\n",
        "    input_data[i,:,0] = Data[i,:,0] # x coordinate (m)\n",
        "    input_data[i,:,1] = Data[i,:,1] # y coordinate (m)\n",
        "    output_data[i,:,0] = Data[i,:,3] # u (m/s)\n",
        "    output_data[i,:,1] = Data[i,:,4] # v (m/s)\n",
        "    output_data[i,:,2] = Data[i,:,2] # p (Pa)"
      ],
      "metadata": {
        "id": "Oq9m3nL6jYzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Normalizing data\n",
        "We normalize the output data (velocity and pressure) in the range of [0, 1] using *Eq. 10* of the [journal paper](https://aip.scitation.org/doi/full/10.1063/5.0033376). Please note that due to this choice, we set the `sigmoid` activation function in the last layer of PointNet. <br>\n",
        "Here, we do not normalize the input data (spatial coordinates, i.e., {*x*, *y*, *z*}). However, one may to normalize the input in the range of [-1, 1]."
      ],
      "metadata": {
        "id": "5qSMC16YNBRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u_min = np.min(output_data[:,:,0])\n",
        "u_max = np.max(output_data[:,:,0])\n",
        "v_min = np.min(output_data[:,:,1])\n",
        "v_max = np.max(output_data[:,:,1])\n",
        "p_min = np.min(output_data[:,:,2])\n",
        "p_max = np.max(output_data[:,:,2])\n",
        "\n",
        "output_data[:,:,0] = (output_data[:,:,0] - u_min)/(u_max - u_min)\n",
        "output_data[:,:,1] = (output_data[:,:,1] - v_min)/(v_max - v_min)\n",
        "output_data[:,:,2] = (output_data[:,:,2] - p_min)/(p_max - p_min)"
      ],
      "metadata": {
        "id": "_L4MQt14yGeZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Data visualization\n",
        "We plot a few of input/output data. If you are using Google Colab, you can find the saved figures in the file sections (left part of the webpage)."
      ],
      "metadata": {
        "id": "mcwFiFJc05iN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot2DPointCloud(x_coord,y_coord,file_name):   \n",
        "    plt.scatter(x_coord,y_coord,s=2.5)\n",
        "    plt.xlabel('x (m)')\n",
        "    plt.ylabel('y (m)')\n",
        "    x_upper = np.max(x_coord) + 1\n",
        "    x_lower = np.min(x_coord) - 1\n",
        "    y_upper = np.max(y_coord) + 1\n",
        "    y_lower = np.min(y_coord) - 1\n",
        "    plt.xlim([x_lower, x_upper])\n",
        "    plt.ylim([y_lower, y_upper])\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "    plt.savefig(file_name+'.png',dpi=300)\n",
        "    #plt.savefig(file_name+'.eps') #You can use this line for saving figures in EPS format   \n",
        "    plt.clf()\n",
        "    #plt.show()\n",
        "\n",
        "def plotSolution(x_coord,y_coord,solution,file_name,title):\n",
        "    plt.scatter(x_coord,y_coord,s=2.5,c=solution,cmap='jet')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('x (m)')\n",
        "    plt.ylabel('y (m)')\n",
        "    x_upper = np.max(x_coord) + 1\n",
        "    x_lower = np.min(x_coord) - 1\n",
        "    y_upper = np.max(y_coord) + 1\n",
        "    y_lower = np.min(y_coord) - 1\n",
        "    plt.xlim([x_lower, x_upper])\n",
        "    plt.ylim([y_lower, y_upper])\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "    cbar= plt.colorbar()\n",
        "    #cbar.set_label(label, labelpad=+1)\n",
        "    plt.savefig(file_name+'.png',dpi=300)\n",
        "    #plt.savefig(file_name+'.eps') #You can use this line for saving figures in EPS format\n",
        "    plt.clf()\n",
        "    #plt.show()\n",
        "    \n",
        "\n",
        "number = 1 #It should be less than number of data ('data_number')\n",
        "plot2DPointCloud(input_data[number,:,0],input_data[number,:,1],'PointCloud')\n",
        "plotSolution(input_data[number,:,0],input_data[number,:,1],output_data[number,:,0],'u_velocity','normalized u (x-velocity component)')\n",
        "plotSolution(input_data[number,:,0],input_data[number,:,1],output_data[number,:,1],'v_velocity','normalized v (y-velocity component)')\n",
        "plotSolution(input_data[number,:,0],input_data[number,:,1],output_data[number,:,2],'pressure','normalized pressure')"
      ],
      "metadata": {
        "id": "I6QGYJTn30WJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Spliting data \n",
        "We split the data *randomly* into three categories of training, validation, and test sets. A reasonable partitioning could be 80% for the training, 10% for validation, and 10% for test sets."
      ],
      "metadata": {
        "id": "zUYbhjvAyoRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.random.permutation(input_data.shape[0])\n",
        "training_idx, validation_idx, test_idx = indices[:int(0.8*data_number)], indices[int(0.8*data_number):int(0.9*data_number)], indices[int(0.9*data_number):]\n",
        "input_training, input_validation, input_test = input_data[training_idx,:], input_data[validation_idx,:], input_data[test_idx,:]\n",
        "output_training, output_validation, output_test = output_data[training_idx,:], output_data[validation_idx,:], output_data[test_idx,:]"
      ],
      "metadata": {
        "id": "KLHlpEJEy0uN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. PointNet architecture\n",
        "We use the segmentation component of PointNet. One of the most important features of PointNet is its scalability. The variable `scaling` in the code allows users to make the network bigger or smaller to control its capacity. Please note that the `sigmoid` activation function is implemented in the last layer, since we normalize the output data in range of [0, 1]. Additionally, we have removed T-Nets (Input Transforms and Feature Transforms) from the network implemented here. However, please note that we had embedded T-Nets in the network in our [journal paper](https://aip.scitation.org/doi/figure/10.1063/5.0033376) (please see *Figure 5* of the journal paper).\n"
      ],
      "metadata": {
        "id": "AT1THU6Gy6Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scaling = 1.0 #reasonable choices for scaling: 4.0, 2.0, 1.0, 0.25, 0.125\n",
        "\n",
        "def exp_dim(global_feature, num_points):\n",
        "    return tf.tile(global_feature, [1, num_points, 1])\n",
        "\n",
        "PointNet_input = Input(shape=(point_numbers, space_variable))\n",
        "#Shared MLP (64,64)\n",
        "branch1 = Convolution1D(int(64*scaling),1,input_shape=(point_numbers,space_variable), activation='relu')(PointNet_input)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "branch1 = Convolution1D(int(64*scaling),1,input_shape=(point_numbers,space_variable), activation='relu')(branch1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "Local_Feature = branch1\n",
        "#Shared MLP (64,128,1024)\n",
        "branch1 = Convolution1D(int(64*scaling),1,activation='relu')(branch1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "branch1 = Convolution1D(int(128*scaling),1,activation='relu')(branch1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "branch1 = Convolution1D(int(1024*scaling),1,activation='relu')(branch1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "#Max function\n",
        "Global_Feature = MaxPooling1D(pool_size=int(point_numbers*scaling))(branch1)\n",
        "Global_Feature = Lambda(exp_dim, arguments={'num_points':point_numbers})(Global_Feature)\n",
        "branch2 = concatenate([Local_Feature, Global_Feature])\n",
        "#Shared MLP (512,256,128)\n",
        "branch2 = Convolution1D(int(512*scaling),1,activation='relu')(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "branch2 = Convolution1D(int(256*scaling),1,activation='relu')(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "branch2 = Convolution1D(int(128*scaling),1,activation='relu')(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "#Shared MLP (128, cfd_variable)\n",
        "branch2 = Convolution1D(int(128*scaling),1,activation='relu')(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "PointNet_output = Convolution1D(cfd_variable,1,activation='sigmoid')(branch2) #Please note that we use the sigmoid activation function in the last layer."
      ],
      "metadata": {
        "id": "kh8Dqo3rzAEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. Defining and compiling the model\n",
        "We use the `Adam` optimizer. Please note to the choice of `learning_rate` and `decaying_rate`. The network is also sensitive to the choice of `epsilon` and it has to be set to a non-zero value. We use the `mean_squared_error` as the loss function (please see *Eq. 15* of the [journal paper](https://aip.scitation.org/doi/full/10.1063/5.0033376)). Please note that for your specific application, you need to tune the `learning_rate` and `decaying_rate`. "
      ],
      "metadata": {
        "id": "7HUZQKmjZ8P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0005\n",
        "decaying_rate = 0.0\n",
        "model = Model(inputs=PointNet_input,outputs=PointNet_output)\n",
        "model.compile(optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=0.000001, decay=decaying_rate)\n",
        "                   , loss='mean_squared_error', metrics=['mean_squared_error'])\n"
      ],
      "metadata": {
        "id": "E6mcnpjDz1zo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12. Training PointNet\n",
        "Please be careful about the choice of batch size (`batch`) and number of epochs (`epoch_number`).<br>\n",
        "At the beginning, you might observe an increase in the validation loss, but please do not worry, it will eventually decrease. <br>\n",
        "Please note that this section might take approximately 20 hours to be completed (when running on Google Colab). So, please be patient. "
      ],
      "metadata": {
        "id": "jIc5SJRZOspb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = 32\n",
        "epoch_number = 2 #4000\n",
        "results = model.fit(input_training,output_training,batch_size=batch,epochs=epoch_number,shuffle=True,verbose=1, validation_split=0.0, validation_data=(input_validation, output_validation))\n",
        " "
      ],
      "metadata": {
        "id": "C9Tyj1Aaz7de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. Plotting training history\n",
        "Trace of loss values over the training and validation set can be seen by plotting the history of training."
      ],
      "metadata": {
        "id": "8yZmXAgEPH55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(results.history['loss'])\n",
        "plt.plot(results.history['val_loss'])\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.savefig('Loss_History.png',dpi=300)\n",
        "#plt.savefig('Loss_History.eps') #You can use this line for saving figures in EPS format  \n",
        "plt.clf()\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "fO1aE0YeoXOr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14. Error analysis\n",
        "We can perform various error analyses. For example, here we are interested in the normalized root mean square error (RMSE). We compute the normalized root mean square error (pointwise) of the predicted velocity and pressure fields over each geometry of the test set, and then we take an average over all these domains. We normalize the velocity error by the free stream velocity, which is 1 m/s. Moreover, we normalized the pressure error by the dynamic pressure (without the factor of 0.5) at the free stream.<br>\n"
      ],
      "metadata": {
        "id": "Og8SUzSmPOSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_RMSE_u = 0\n",
        "average_RMSE_v = 0\n",
        "average_RMSE_p = 0\n",
        "\n",
        "test_set_number = test_idx.size\n",
        "sample_point_cloud = zeros([1,point_numbers,space_variable],dtype='f')\n",
        "truth_point_cloud = zeros([point_numbers,cfd_variable],dtype='f')\n",
        "\n",
        "for s in range(test_set_number):\n",
        "    for j in range(point_numbers):\n",
        "        for i in range(space_variable):\n",
        "            sample_point_cloud[0][j][i] = input_test[s][j][i]\n",
        "\n",
        "    prediction = model.predict(sample_point_cloud, batch_size=None, verbose=0)\n",
        "    \n",
        "    #Unnormalized\n",
        "    prediction[0,:,0] = prediction[0,:,0]*(u_max - u_min) + u_min \n",
        "    prediction[0,:,1] = prediction[0,:,1]*(v_max - v_min) + v_min\n",
        "    prediction[0,:,2] = prediction[0,:,2]*(p_max - p_min) + p_min \n",
        "\n",
        "    output_test[s,:,0] = output_test[s,:,0]*(u_max - u_min) + u_min \n",
        "    output_test[s,:,1] = output_test[s,:,1]*(v_max - v_min) + v_min\n",
        "    output_test[s,:,2] = output_test[s,:,2]*(p_max - p_min) + p_min\n",
        "\n",
        "    average_RMSE_u += np.sqrt(np.sum(np.square(prediction[0,:,0]-output_test[s,:,0])))/point_numbers\n",
        "    average_RMSE_v += np.sqrt(np.sum(np.square(prediction[0,:,1]-output_test[s,:,1])))/point_numbers\n",
        "    average_RMSE_p += np.sqrt(np.sum(np.square(prediction[0,:,2]-output_test[s,:,2])))/point_numbers\n",
        "    \n",
        "\n",
        "average_RMSE_u = average_RMSE_u/test_set_number\n",
        "average_RMSE_v = average_RMSE_v/test_set_number\n",
        "average_RMSE_p = average_RMSE_p/test_set_number\n",
        "\n",
        "print('Average normalized RMSE of the x-velocity component (u) over goemtries of the test set:')\n",
        "print(average_RMSE_u)\n",
        "print('Average normalized RMSE of the y-velocity component (v) over goemtries of the test set:')\n",
        "print(average_RMSE_v)\n",
        "print('Average normalized RMSE of the pressure (p) over goemtries of the test set:')\n",
        "print(average_RMSE_p)\n"
      ],
      "metadata": {
        "id": "Lth0TI-33USM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15. Visualizing the results\n",
        "For example, let us plot the velocity and pressure fields predicted by the network as well as absolute pointwise error distribution over these fields for one of the geometries of the test set."
      ],
      "metadata": {
        "id": "0xa4X6vfPuLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s=0 #s can change from \"0\" to \"test_set_number - 1\"\n",
        "\n",
        "for j in range(point_numbers):\n",
        "    for i in range(space_variable):\n",
        "        sample_point_cloud[0][j][i] = input_test[s][j][i]\n",
        "\n",
        "prediction = model.predict(sample_point_cloud, batch_size=None, verbose=0)\n",
        "    \n",
        "#Unnormalized\n",
        "prediction[0,:,0] = prediction[0,:,0]*(u_max - u_min) + u_min \n",
        "prediction[0,:,1] = prediction[0,:,1]*(v_max - v_min) + v_min\n",
        "prediction[0,:,2] = prediction[0,:,2]*(p_max - p_min) + p_min \n",
        "\n",
        "#Please note that we have already unnormalized the 'output_test' in Sect. 14 Error Analysis.\n",
        "\n",
        "plotSolution(sample_point_cloud[0,:,0],sample_point_cloud[0,:,1],prediction[0,:,0],'u_prediction','velocity prediction (u) m/s')\n",
        "plotSolution(sample_point_cloud[0,:,0],sample_point_cloud[0,:,1],output_test[s,:,0],'u_ground_truth','velocity ground truth (u) m/s')\n",
        "plotSolution(sample_point_cloud[0,:,0],sample_point_cloud[0,:,1],np.absolute(output_test[s,:,0]-prediction[0,:,0]),'u_point_wise_error','absolute point-wise error of velocity (u) m/s')\n",
        "\n",
        "plotSolution(sample_point_cloud[0,:,0],sample_point_cloud[0,:,1],prediction[0,:,1],'v_prediction','velocity prediction (v) m/s')\n",
        "plotSolution(sample_point_cloud[0,:,0],sample_point_cloud[0,:,1],output_test[s,:,1],'v_ground_truth','velocity ground truth (v) m/s')\n",
        "plotSolution(sample_point_cloud[0,:,0],sample_point_cloud[0,:,1],np.absolute(output_test[s,:,1]-prediction[0,:,1]),'v_point_wise_error','absolute point-wise error of velocity (v) m/s')\n",
        "\n",
        "plotSolution(sample_point_cloud[0,:,0],sample_point_cloud[0,:,1],prediction[0,:,2],'p_prediction','pressure prediction (p) Pa')\n",
        "plotSolution(sample_point_cloud[0,:,0],sample_point_cloud[0,:,1],output_test[s,:,2],'p_ground_truth','pressure ground truth (u) m/s')\n",
        "plotSolution(sample_point_cloud[0,:,0],sample_point_cloud[0,:,1],np.absolute(output_test[s,:,2]-prediction[0,:,2]),'p_point_wise_error','absolute point-wise error of pressure (p) Pa')"
      ],
      "metadata": {
        "id": "srB6jz5_7Aqe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16. Questions?\n",
        "If you have any questions or need assistance, please do not hesitate to contact Ali Kashefi (kashefi@stanford.edu) or Davis Rempe (drempe@stanford.edu) via email. "
      ],
      "metadata": {
        "id": "ZPE3ngrisCyo"
      }
    }
  ]
}
